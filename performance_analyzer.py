#!/usr/bin/env python3
"""
Ù…Ø­Ù„Ù„ Ø§Ù„Ø£Ø¯Ø§Ø¡ - Performance Analyzer
Ù†Ø¸Ø§Ù… Ø¨ØµÙŠØ±Ø© Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„

ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…
âš¡ Ù‚ÙŠØ§Ø³ Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„ÙƒÙØ§Ø¡Ø©
ğŸ¯ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡

Ø§Ù„Ù…Ø·ÙˆØ±: Ø¨Ø§Ø³Ù„ ÙŠØ­ÙŠÙ‰ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡
Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ÙÙƒØ§Ø± ÙˆØ§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ù…Ù† Ø¥Ø¨Ø¯Ø§Ø¹ Ø¨Ø§Ø³Ù„ ÙŠØ­ÙŠÙ‰ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡
"""

import time
import statistics
from typing import Dict, List, Any, Callable
from dataclasses import dataclass
from datetime import datetime

@dataclass
class PerformanceResult:
    """Ù†ØªÙŠØ¬Ø© Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    function_name: str
    execution_time: float
    memory_usage: float
    success: bool
    error_message: str = ""

class BaseraPerformanceAnalyzer:
    """
    Ù…Ø­Ù„Ù„ Ø£Ø¯Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø¨ØµÙŠØ±Ø©
    
    ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ø£Ø¯Ø§Ø¡:
    - Ù‚ÙŠØ§Ø³ Ø£ÙˆÙ‚Ø§Øª Ø§Ù„ØªÙ†ÙÙŠØ°
    - Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    - ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙØ§Ø¡Ø©
    - ØªÙ‚Ø§Ø±ÙŠØ± Ù…ÙØµÙ„Ø©
    """
    
    def __init__(self):
        self.creation_time = datetime.now()
        self.performance_results: List[PerformanceResult] = []
        
        print(f"ğŸ“Šâš¡ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ù„Ù„ Ø£Ø¯Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø¨ØµÙŠØ±Ø©")
        print(f"   ğŸ• ÙˆÙ‚Øª Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡: {self.creation_time}")
    
    def measure_performance(self, func: Callable, *args, **kwargs) -> PerformanceResult:
        """Ù‚ÙŠØ§Ø³ Ø£Ø¯Ø§Ø¡ Ø¯Ø§Ù„Ø©"""
        import tracemalloc
        
        function_name = func.__name__ if hasattr(func, '__name__') else str(func)
        
        # Ø¨Ø¯Ø¡ Ù‚ÙŠØ§Ø³ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        tracemalloc.start()
        
        start_time = time.time()
        success = True
        error_message = ""
        
        try:
            # ØªÙ†ÙÙŠØ° Ø§Ù„Ø¯Ø§Ù„Ø©
            result = func(*args, **kwargs)
            
        except Exception as e:
            success = False
            error_message = str(e)
            result = None
        
        execution_time = time.time() - start_time
        
        # Ù‚ÙŠØ§Ø³ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        memory_usage = peak / 1024 / 1024  # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ MB
        
        performance_result = PerformanceResult(
            function_name=function_name,
            execution_time=execution_time,
            memory_usage=memory_usage,
            success=success,
            error_message=error_message
        )
        
        self.performance_results.append(performance_result)
        
        print(f"âš¡ Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡: {function_name}")
        print(f"   â±ï¸ Ø§Ù„ÙˆÙ‚Øª: {execution_time:.4f}s")
        print(f"   ğŸ’¾ Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {memory_usage:.2f}MB")
        print(f"   âœ… Ø§Ù„Ù†Ø¬Ø§Ø­: {success}")
        
        return performance_result
    
    def benchmark_function(self, func: Callable, iterations: int = 10, *args, **kwargs) -> Dict[str, Any]:
        """Ù‚ÙŠØ§Ø³ Ø£Ø¯Ø§Ø¡ Ø¯Ø§Ù„Ø© Ø¹Ø¯Ø© Ù…Ø±Ø§Øª"""
        print(f"ğŸƒ Ø¨Ø¯Ø¡ Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ØªÙƒØ±Ø±: {func.__name__} ({iterations} Ù…Ø±Ø§Øª)")
        
        execution_times = []
        memory_usages = []
        successes = 0
        
        for i in range(iterations):
            result = self.measure_performance(func, *args, **kwargs)
            
            if result.success:
                execution_times.append(result.execution_time)
                memory_usages.append(result.memory_usage)
                successes += 1
        
        if not execution_times:
            return {
                "function_name": func.__name__,
                "iterations": iterations,
                "success_rate": 0.0,
                "error": "Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª ÙØ´Ù„Øª"
            }
        
        benchmark_result = {
            "function_name": func.__name__,
            "iterations": iterations,
            "success_rate": successes / iterations * 100,
            "execution_time": {
                "min": min(execution_times),
                "max": max(execution_times),
                "mean": statistics.mean(execution_times),
                "median": statistics.median(execution_times),
                "stdev": statistics.stdev(execution_times) if len(execution_times) > 1 else 0
            },
            "memory_usage": {
                "min": min(memory_usages),
                "max": max(memory_usages),
                "mean": statistics.mean(memory_usages),
                "median": statistics.median(memory_usages)
            }
        }
        
        print(f"ğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ØªÙƒØ±Ø±:")
        print(f"   âœ… Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {benchmark_result['success_rate']:.1f}%")
        print(f"   â±ï¸ Ù…ØªÙˆØ³Ø· Ø§Ù„ÙˆÙ‚Øª: {benchmark_result['execution_time']['mean']:.4f}s")
        print(f"   ğŸ’¾ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {benchmark_result['memory_usage']['mean']:.2f}MB")
        
        return benchmark_result
    
    def compare_functions(self, functions: List[Callable], iterations: int = 5, *args, **kwargs) -> Dict[str, Any]:
        """Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ø¯Ø§Ø¡ Ø¹Ø¯Ø© Ø¯ÙˆØ§Ù„"""
        print(f"ğŸ”„ Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ø¯Ø§Ø¡ {len(functions)} Ø¯Ø§Ù„Ø©")
        
        comparison_results = {}
        
        for func in functions:
            benchmark = self.benchmark_function(func, iterations, *args, **kwargs)
            comparison_results[func.__name__] = benchmark
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­Ø³Ø¨ Ø§Ù„Ø³Ø±Ø¹Ø©
        sorted_by_speed = sorted(
            comparison_results.items(),
            key=lambda x: x[1]['execution_time']['mean'] if x[1].get('execution_time') else float('inf')
        )
        
        print(f"\nğŸ† ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø­Ø³Ø¨ Ø§Ù„Ø³Ø±Ø¹Ø©:")
        for i, (name, result) in enumerate(sorted_by_speed, 1):
            if result.get('execution_time'):
                print(f"   {i}. {name}: {result['execution_time']['mean']:.4f}s")
            else:
                print(f"   {i}. {name}: ÙØ´Ù„")
        
        return {
            "comparison_results": comparison_results,
            "fastest_function": sorted_by_speed[0][0] if sorted_by_speed and sorted_by_speed[0][1].get('execution_time') else None,
            "performance_ranking": [name for name, _ in sorted_by_speed]
        }
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Ù…Ù„Ø®Øµ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù…"""
        if not self.performance_results:
            return {"error": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ø£Ø¯Ø§Ø¡"}
        
        total_tests = len(self.performance_results)
        successful_tests = sum(1 for r in self.performance_results if r.success)
        
        execution_times = [r.execution_time for r in self.performance_results if r.success]
        memory_usages = [r.memory_usage for r in self.performance_results if r.success]
        
        summary = {
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "success_rate": successful_tests / total_tests * 100,
            "testing_duration": str(datetime.now() - self.creation_time)
        }
        
        if execution_times:
            summary["performance_stats"] = {
                "avg_execution_time": statistics.mean(execution_times),
                "avg_memory_usage": statistics.mean(memory_usages),
                "fastest_execution": min(execution_times),
                "slowest_execution": max(execution_times)
            }
        
        return summary
    
    def generate_performance_report(self) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø£Ø¯Ø§Ø¡ Ù…ÙØµÙ„"""
        summary = self.get_performance_summary()
        
        report = f"""
ğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø£Ø¯Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø¨ØµÙŠØ±Ø© Ø§Ù„Ø«ÙˆØ±ÙŠ
{'='*50}

ğŸ“ˆ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©:
   ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª: {summary.get('total_tests', 0)}
   âœ… Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø©: {summary.get('successful_tests', 0)}
   ğŸ“Š Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {summary.get('success_rate', 0):.1f}%
   â±ï¸ Ù…Ø¯Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {summary.get('testing_duration', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}

"""
        
        if summary.get('performance_stats'):
            stats = summary['performance_stats']
            report += f"""âš¡ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡:
   â±ï¸ Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„ØªÙ†ÙÙŠØ°: {stats['avg_execution_time']:.4f}s
   ğŸ’¾ Ù…ØªÙˆØ³Ø· Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {stats['avg_memory_usage']:.2f}MB
   ğŸš€ Ø£Ø³Ø±Ø¹ ØªÙ†ÙÙŠØ°: {stats['fastest_execution']:.4f}s
   ğŸŒ Ø£Ø¨Ø·Ø£ ØªÙ†ÙÙŠØ°: {stats['slowest_execution']:.4f}s

"""
        
        # ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„ÙØ±Ø¯ÙŠØ©
        report += "ğŸ“‹ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª:\n"
        for i, result in enumerate(self.performance_results[-10:], 1):  # Ø¢Ø®Ø± 10 Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
            status = "âœ…" if result.success else "âŒ"
            report += f"   {i}. {status} {result.function_name}: {result.execution_time:.4f}s, {result.memory_usage:.2f}MB\n"
        
        report += f"\nğŸ§¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ÙÙƒØ§Ø± ÙˆØ§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ù…Ù† Ø¥Ø¨Ø¯Ø§Ø¹ Ø¨Ø§Ø³Ù„ ÙŠØ­ÙŠÙ‰ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡"
        
        return report

def test_performance_analyzer():
    """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ù„Ù„ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    analyzer = BaseraPerformanceAnalyzer()
    
    # Ø¯ÙˆØ§Ù„ Ø§Ø®ØªØ¨Ø§Ø±
    def fast_function():
        return sum(range(1000))
    
    def slow_function():
        time.sleep(0.1)
        return sum(range(10000))
    
    def memory_intensive_function():
        data = [i**2 for i in range(100000)]
        return len(data)
    
    # Ù‚ÙŠØ§Ø³ Ø£Ø¯Ø§Ø¡ ÙØ±Ø¯ÙŠ
    analyzer.measure_performance(fast_function)
    analyzer.measure_performance(slow_function)
    analyzer.measure_performance(memory_intensive_function)
    
    # Ù‚ÙŠØ§Ø³ Ù…ØªÙƒØ±Ø±
    analyzer.benchmark_function(fast_function, iterations=5)
    
    # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¯ÙˆØ§Ù„
    analyzer.compare_functions([fast_function, slow_function], iterations=3)
    
    # ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡
    print(analyzer.generate_performance_report())

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    test_performance_analyzer()

if __name__ == "__main__":
    main()

